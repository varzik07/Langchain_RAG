{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae2ad41-9039-41c7-ba26-f80b5759cf48",
   "metadata": {},
   "source": [
    "## Hybrid Retriever- Combining Dense And Sparse Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24b6fcd3-7b89-4ed8-9368-68936f4be8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7d7054a-278b-45d5-abbe-f7ad5e428032",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"Pipecode is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"LangChain can be used to developed agentic AI application.\"),\n",
    "    Document(page_content=\"LangChain has many types of retrievers.\"),\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\")\n",
    "    \n",
    "]\n",
    "\n",
    "## Step 2: Dense Retriever (FAISS + HuggingFace)\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore  = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c34b534-2b36-48d6-87f2-8ca50aed13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = dense_vectorstore.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fe99708-077a-4502-98df-15ee36d62fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 384)\n",
      "[[-0.01785619 -0.02074999  0.04232785 ... -0.0190144   0.05980122\n",
      "   0.02640989]\n",
      " [ 0.02465541 -0.00338172 -0.0284394  ...  0.09900173  0.03497263\n",
      "  -0.0354636 ]\n",
      " [ 0.06605352  0.03884847  0.01661565 ...  0.03093835  0.07990999\n",
      "   0.05157552]\n",
      " [-0.06784829 -0.01717511 -0.03610207 ...  0.0658505   0.06786532\n",
      "  -0.02381885]\n",
      " [-0.06612371  0.00435344  0.01951677 ...  0.01170399  0.09943537\n",
      "   0.06213933]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_docs = len(docs)\n",
    "\n",
    "# Reconstruct all vectors\n",
    "vectors = np.array([faiss_index.reconstruct(i) for i in range(num_docs)])\n",
    "print(vectors.shape)  \n",
    "print(vectors)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c62c6764-765e-4a0b-9338-815671812f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sparse Retriever(BM25)\n",
    "sparse_retriever  = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k = 3\n",
    "\n",
    "## step 4: Combine with Ensemble retriever\n",
    "\n",
    "hybrid_retriever  = EnsembleRetriever(\n",
    "    retrievers=[ dense_retriever,sparse_retriever ],\n",
    "    weights=[0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43b22ae9-61f6-4823-b707-9accf9e6db69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f5ae3dc3110>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x7f5ae3dc3610>, k=3)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07359a8a-a104-4ca1-8158-968786f0448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Document 1:\n",
      " LangChain helps build LLM applications.\n",
      "\n",
      " Document 2:\n",
      " LangChain can be used to developed agentic AI application.\n",
      "\n",
      " Document 3:\n",
      " LangChain has many types of retrievers.\n",
      "\n",
      " Document 4:\n",
      " Pipecode is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "## Step 5: Query and get results\n",
    "\n",
    "query   = \"How can I build an application using LLms\"\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "## Step 6: print result\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n Document {i+1}:\\n {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbfd074-b5ce-4778-b2e9-b4d70655e349",
   "metadata": {},
   "source": [
    "## RAG Pipline with hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d85d6184-554f-4df9-a0f2-81aebde6015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b71c7c7b-0980-4493-98f9-d8446e5c6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb96125e-3009-488a-a9cd-12c7b5880a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7f5ae041d7f0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f5ae041e270>, root_client=<openai.OpenAI object at 0x7f5ae07a74d0>, root_async_client=<openai.AsyncOpenAI object at 0x7f5ae041dfd0>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 5: Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "## Step 6: lLm\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-3.5-turbo\", temperature = 0.2)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92ae9022-7f73-489d-8ac6-a4ba35d9f3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f5ae3dc3110>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x7f5ae3dc3610>, k=3)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {input}\\n')\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7f5ae041d7f0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f5ae041e270>, root_client=<openai.OpenAI object at 0x7f5ae07a74d0>, root_async_client=<openai.AsyncOpenAI object at 0x7f5ae041dfd0>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create stuff Document Chain\n",
    "\n",
    "document_chain  = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "## Create full RAG chain\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever=hybrid_retriever, combine_docs_chain=document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e3f9ceb-55ff-4d1c-b2c9-9fa468ca49af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " You can build an app using LLMs by utilizing LangChain, which helps in developing LLM applications. LangChain can be used to create agentic AI applications and has various types of retrievers that can be utilized in building your app. Additionally, you can also consider using Pipecode, a vector database for semantic search, to enhance the functionality of your app.\n",
      "\n",
      " Sourse Documents:\n",
      "\n",
      " Doc 1: LangChain helps build LLM applications.\n",
      "\n",
      " Doc 2: LangChain can be used to developed agentic AI application.\n",
      "\n",
      " Doc 3: LangChain has many types of retrievers.\n",
      "\n",
      " Doc 4: Pipecode is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "## Step 9: Ask a question\n",
    "\n",
    "query = {\"input\":\"How can I build an app using LLMs?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(\"Answer:\\n\", response['answer'])\n",
    "\n",
    "print(\"\\n Sourse Documents:\")\n",
    "\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\n Doc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4ae8c7e-811a-4ccd-b014-770142c26a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['LangChain helps build LLM applications.', 0],\n",
       " ['LangChain can be used to developed agentic AI application.', 1],\n",
       " ['LangChain has many types of retrievers.', 2],\n",
       " ['Pipecode is a vector database for semantic search.', 3]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[response[\"context\"][i].page_content, i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d704a5-2f40-44f7-bcdd-63ffecaa716b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my07",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
