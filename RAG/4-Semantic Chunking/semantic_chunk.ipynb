{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd71068-16ee-4ac2-a6ec-4cb9d7deea59",
   "metadata": {},
   "source": [
    "### Semantic Chunking\n",
    "- SemanticChunker is a document splitter that uses embedding similarity between sentences to decide chunk boundaries.\n",
    "\n",
    "- It ensures that each chunk is semantically coherent and not cut off mid-thought like traditional character/token splitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "296b10f7-fdeb-4c66-93b0-ccbd62f9cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/oybek/PKD/xudamadarsam/mashq/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableMap\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d1a6c0d-28ab-4adf-9661-6eacd3bd781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9118decf-cfdc-40fa-88ec-7a89e0a1188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the model\n",
    "model  = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "## Sample text\n",
    "text=\"\"\"\n",
    "LangChain is a framework for building applications with LLMs.\n",
    "Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf489e71-cd8f-4d2a-8bd4-e037b602436c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"\"\"\n",
    "LangChain is a powerful open-source framework designed for building applications that integrate Large Language Models (LLMs) with external data and tools. It provides a modular architecture that simplifies connecting models like OpenAI’s GPT, Anthropic’s Claude, or Meta’s LLaMA with components such as databases, APIs, and vector stores.\n",
    "\n",
    "LangChain introduces key concepts including Chains, Agents, Retrievers, and Memory.\n",
    "\n",
    "A Chain is a sequence of operations or prompts that process input and produce output.\n",
    "\n",
    "An Agent uses reasoning to decide which tool or action to execute next based on model output.\n",
    "\n",
    "A Retriever fetches the most relevant information from a data source, often using embeddings and vector similarity search.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "sentences = [s.strip() for s in text1.split(\"\\n\") if s.strip()]\n",
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48317395-baca-4df9-8432-eb92c22cf873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6541584\n",
      "0.40541422\n",
      "0.37255263\n",
      "0.16015358\n",
      "\n",
      " Chunk 1: \n",
      " LangChain is a powerful open-source framework designed for building applications that integrate Large Language Models (LLMs) with external data and tools. It provides a modular architecture that simplifies connecting models like OpenAI’s GPT, Anthropic’s Claude, or Meta’s LLaMA with components such as databases, APIs, and vector stores.\n",
      "\n",
      " Chunk 2: \n",
      " LangChain introduces key concepts including Chains, Agents, Retrievers, and Memory.\n",
      "\n",
      " Chunk 3: \n",
      " A Chain is a sequence of operations or prompts that process input and produce output.\n",
      "\n",
      " Chunk 4: \n",
      " An Agent uses reasoning to decide which tool or action to execute next based on model output.\n",
      "\n",
      " Chunk 5: \n",
      " A Retriever fetches the most relevant information from a data source, often using embeddings and vector similarity search.\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Split into sentences\n",
    "\n",
    "sentences = [s.strip() for s in text1.split(\"\\n\") if s.strip()]\n",
    "\n",
    "\n",
    "## Step 2: Embed each sntence\n",
    "\n",
    "embedding = model.encode(sentences)\n",
    "\n",
    "## Step 3: initialize parameters\n",
    "threshold = 0.7\n",
    "chunks = []\n",
    "current_chunk = [sentences[0]]\n",
    "\n",
    "\n",
    "## Step 4: Semantic grouping based on threshold\n",
    "for i in range(1, len(sentences)):\n",
    "    sim = cosine_similarity(\n",
    "        [embedding[i-1]],\n",
    "        [embedding[i]]\n",
    "    )[0][0]\n",
    "    print(sim)\n",
    "    if sim>threshold:\n",
    "        current_chunk.append(sentences[i])\n",
    "    else:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk = [sentences[i]]\n",
    "\n",
    "## Append the last chunk\n",
    "chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"\\n Chunk {idx+1}: \\n {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0be07ca-33dc-4adb-b083-a0156a874247",
   "metadata": {},
   "source": [
    "## RAG Pipeline Modular DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f5485e1-faaf-4942-9309-2fc67ad6c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Semantic Chunker With Threshold\n",
    "\n",
    "class ThresholSemanticChunker:\n",
    "    def __init__(self, model_name = \"all-MiniLM-L6-v2\", threshold = 0.7):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def split(self, text:str):\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        embedding = self.model.encode(sentences)\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "\n",
    "        for i in range(1, len(sentences)):\n",
    "            sim = cosine_similarity([embedding[i-1]],[embedding[i]])\n",
    "            if sim>threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk)+ '.')\n",
    "                current_chunk = [sentences[i]]\n",
    "        chunks.append('.'.join(current_chunk) + '.')\n",
    "        return chunks\n",
    "\n",
    "    def split_documents(self, docs):\n",
    "        result = []\n",
    "        for doc in docs:\n",
    "            for chunk in self.split(doc.page_content):\n",
    "                result.append(Document(page_content = chunk, metadata = doc.metadata))\n",
    "        return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1843ed01-6af2-4884-9255-85ec6a44c88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='\\nLangChain is a framework for building applications with LLMs.\\nLangchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\\nYou can create chains, agents, memory, and retrievers.\\nThe Eiffel Tower is located in Paris.\\nFrance is a popular tourist destination.\\n')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Documnet\n",
    "\n",
    "doc = Document(page_content=text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d3c752d-ca4f-473e-89db-e7fe93cd7840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='LangChain is a framework for building applications with LLMs Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.'),\n",
       " Document(metadata={}, page_content='You can create chains, agents, memory, and retrievers.'),\n",
       " Document(metadata={}, page_content='The Eiffel Tower is located in Paris.'),\n",
       " Document(metadata={}, page_content='France is a popular tourist destination.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunker = ThresholSemanticChunker(threshold = 0.7)\n",
    "chunks = chunker.split_documents([doc])\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97529838-ee60-4391-a629-658bad214b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VectorStore\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorStore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorStore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74c77b0d-7cc1-497b-afbc-c23286283cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=' Answer the question based on the following context:\\n    {context}\\n    Question: {question}\\n    ')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prompt Template\n",
    "\n",
    "template = \"\"\" Answer the question based on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4c9390b-c0b8-4412-a6f9-f413b44a544d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a framework used for building applications with Large Language Models (LLMs). It provides modular abstractions that allow LLMs to be combined with various tools, such as OpenAI and Pinecone. Additionally, it enables the creation of chains, agents, memory, and retrievers, suggesting a comprehensive toolkit for developing complex applications that leverage LLMs.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LLM \n",
    "# llm = init_chat_model(model = \"groq:gemma2-9b-it\", temperature = 0.4)\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"meta-llama/llama-4-maverick-17b-128e-instruct\", api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "## LCEL Chain With retrieval\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableMap(\n",
    "        {\n",
    "            \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "        }\n",
    "    )\n",
    "    |prompt\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    "    \n",
    ")\n",
    "\n",
    "## Run Query\n",
    "\n",
    "query = {\"question\": \"What is LangChain used for?\"}\n",
    "result = rag_chain.invoke(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd43924-3915-4362-bdaf-63a124c70812",
   "metadata": {},
   "source": [
    "## Semantic chunker with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9aae7568-9d51-4c44-95e8-51453ecef7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ecf910cc-1687-4e75-94ea-0fcea9a53c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Chunk 1: \n",
      "LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
      "\n",
      " Chunk 2: \n",
      "You can create chains, agents, memory, and retrievers. The Eiffel Tower is located in Paris. France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "## Load the documents\n",
    "loader = TextLoader(\"langchain_intro.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "## Initialize embedding model\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "## Create the semantic chunker\n",
    "chunker = SemanticChunker(embedding)\n",
    "\n",
    "## Split the documents\n",
    "chunks = chunker.split_documents(docs)\n",
    "\n",
    "## result\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n Chunk {i+1}: \\n{chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d46ed56e-c1e8-4865-9c35-593529691164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_experimental.text_splitter.SemanticChunker object at 0x7f2568797790>\n",
      "\n",
      " Chunk 1: \n",
      "FAISS (Facebook AI Similarity Search) is an open-source library. It supports CPU and GPU acceleration for high-dimensional data.\n",
      "\n",
      " Chunk 2: \n",
      "FAISS is a core component in many RAG pipelines. It is used for efficient similarity search and clustering of vectors.\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"langchain_intro.txt.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "## Initialize embedding model\n",
    "embedding = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "## Create the semantic chunker\n",
    "chunker = SemanticChunker(embedding, breakpoint_threshold_type=\"percentile\", breakpoint_threshold_amount=70)\n",
    "print(chunker)\n",
    "\n",
    "\n",
    "## Split the documents\n",
    "chunks = chunker.split_documents(docs)\n",
    "\n",
    "## result\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n Chunk {i+1}: \\n{chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535333f0-b6c8-4f67-9e21-c41e2244106e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my07",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
